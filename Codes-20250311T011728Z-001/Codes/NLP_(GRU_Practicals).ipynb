{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pgt2bbFUr2gZ"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Data plays a vital role in our everyday life.\n",
        "Directly or indirectly, for daily life decisions, we depend on some data, be it choosing a novel to read from a list of books, buying a thing after considering the budget, and so on.\n",
        "Have you ever imagined searching for something on Google or Yahoo generates a lot of data?\n",
        "This data is essential to analyze user experiences.\n",
        "Getting recommendations on various e-commerce websites after buying a product and tracking parcels during delivery are part of Data Analytics which involves analyzing the raw data to make informed decisions.\n",
        "But this raw data does not help make decisions if it has some redundancy, inconsistency, or inaccuracy.\n",
        "Therefore, this data needs to be cleaned before considering for analysis.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "hRY9s1CNwyCC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "PW-VY5avMM-k"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "KKzj7JNzMSSJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8myerEMVTB",
        "outputId": "2dbfefa2-a0a2-47e1-8a71-6a8e0b4ef18a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in text.split('\\n'):\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRl6t-h0NRNw",
        "outputId": "8e43fe3e-b9e9-4cd1-ab8c-794839488b46"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data plays a vital role in our everyday life.\n",
            "Directly or indirectly, for daily life decisions, we depend on some data, be it choosing a novel to read from a list of books, buying a thing after considering the budget, and so on.\n",
            "Have you ever imagined searching for something on Google or Yahoo generates a lot of data?\n",
            "This data is essential to analyze user experiences.\n",
            "Getting recommendations on various e-commerce websites after buying a product and tracking parcels during delivery are part of Data Analytics which involves analyzing the raw data to make informed decisions.\n",
            "But this raw data does not help make decisions if it has some redundancy, inconsistency, or inaccuracy.\n",
            "Therefore, this data needs to be cleaned before considering for analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in text.split('\\n'):\n",
        "  print(tokenizer.texts_to_sequences([sentence])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkRWZeniMhH6",
        "outputId": "5d5f8c5f-ed1c-45a8-e548-466120ee02e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 21, 2, 22, 23, 24, 25, 26, 10]\n",
            "[27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33, 4, 34, 35, 2, 36, 8, 37, 14, 2, 38, 15, 16, 17, 39, 18, 40, 3]\n",
            "[41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49, 2, 50, 8, 1]\n",
            "[9, 1, 51, 52, 4, 53, 54, 55]\n",
            "[56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66, 67, 68, 8, 1, 69, 70, 71, 72, 17, 19, 1, 4, 20, 73, 7]\n",
            "[74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11, 80, 81, 5, 82]\n",
            "[83, 9, 1, 84, 4, 12, 85, 86, 16, 6, 87]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "\n",
        "for sentence in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1, len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "6J5y6cYCMrMB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yJ1jTKgOJoB",
        "outputId": "2336cf98-98f7-4bc7-b84f-9dc02987ad8c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 21],\n",
              " [1, 21, 2],\n",
              " [1, 21, 2, 22],\n",
              " [1, 21, 2, 22, 23],\n",
              " [1, 21, 2, 22, 23, 24],\n",
              " [1, 21, 2, 22, 23, 24, 25],\n",
              " [1, 21, 2, 22, 23, 24, 25, 26],\n",
              " [1, 21, 2, 22, 23, 24, 25, 26, 10],\n",
              " [27, 5],\n",
              " [27, 5, 28],\n",
              " [27, 5, 28, 6],\n",
              " [27, 5, 28, 6, 29],\n",
              " [27, 5, 28, 6, 29, 10],\n",
              " [27, 5, 28, 6, 29, 10, 7],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33, 4],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33, 4, 34],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33, 4, 34, 35],\n",
              " [27, 5, 28, 6, 29, 10, 7, 30, 31, 3, 11, 1, 12, 13, 32, 2, 33, 4, 34, 35, 2],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16,\n",
              "  17],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  39],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  39,\n",
              "  18],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  39,\n",
              "  18,\n",
              "  40],\n",
              " [27,\n",
              "  5,\n",
              "  28,\n",
              "  6,\n",
              "  29,\n",
              "  10,\n",
              "  7,\n",
              "  30,\n",
              "  31,\n",
              "  3,\n",
              "  11,\n",
              "  1,\n",
              "  12,\n",
              "  13,\n",
              "  32,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  34,\n",
              "  35,\n",
              "  2,\n",
              "  36,\n",
              "  8,\n",
              "  37,\n",
              "  14,\n",
              "  2,\n",
              "  38,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  39,\n",
              "  18,\n",
              "  40,\n",
              "  3],\n",
              " [41, 42],\n",
              " [41, 42, 43],\n",
              " [41, 42, 43, 44],\n",
              " [41, 42, 43, 44, 45],\n",
              " [41, 42, 43, 44, 45, 6],\n",
              " [41, 42, 43, 44, 45, 6, 46],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49, 2],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49, 2, 50],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49, 2, 50, 8],\n",
              " [41, 42, 43, 44, 45, 6, 46, 3, 47, 5, 48, 49, 2, 50, 8, 1],\n",
              " [9, 1],\n",
              " [9, 1, 51],\n",
              " [9, 1, 51, 52],\n",
              " [9, 1, 51, 52, 4],\n",
              " [9, 1, 51, 52, 4, 53],\n",
              " [9, 1, 51, 52, 4, 53, 54],\n",
              " [9, 1, 51, 52, 4, 53, 54, 55],\n",
              " [56, 57],\n",
              " [56, 57, 3],\n",
              " [56, 57, 3, 58],\n",
              " [56, 57, 3, 58, 59],\n",
              " [56, 57, 3, 58, 59, 60],\n",
              " [56, 57, 3, 58, 59, 60, 61],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66, 67],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66, 67, 68],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66, 67, 68, 8],\n",
              " [56, 57, 3, 58, 59, 60, 61, 15, 14, 2, 62, 18, 63, 64, 65, 66, 67, 68, 8, 1],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19,\n",
              "  1],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19,\n",
              "  1,\n",
              "  4],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19,\n",
              "  1,\n",
              "  4,\n",
              "  20],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19,\n",
              "  1,\n",
              "  4,\n",
              "  20,\n",
              "  73],\n",
              " [56,\n",
              "  57,\n",
              "  3,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  15,\n",
              "  14,\n",
              "  2,\n",
              "  62,\n",
              "  18,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  8,\n",
              "  1,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  17,\n",
              "  19,\n",
              "  1,\n",
              "  4,\n",
              "  20,\n",
              "  73,\n",
              "  7],\n",
              " [74, 9],\n",
              " [74, 9, 19],\n",
              " [74, 9, 19, 1],\n",
              " [74, 9, 19, 1, 75],\n",
              " [74, 9, 19, 1, 75, 76],\n",
              " [74, 9, 19, 1, 75, 76, 77],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11, 80],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11, 80, 81],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11, 80, 81, 5],\n",
              " [74, 9, 19, 1, 75, 76, 77, 20, 7, 78, 13, 79, 11, 80, 81, 5, 82],\n",
              " [83, 9],\n",
              " [83, 9, 1],\n",
              " [83, 9, 1, 84],\n",
              " [83, 9, 1, 84, 4],\n",
              " [83, 9, 1, 84, 4, 12],\n",
              " [83, 9, 1, 84, 4, 12, 85],\n",
              " [83, 9, 1, 84, 4, 12, 85, 86],\n",
              " [83, 9, 1, 84, 4, 12, 85, 86, 16],\n",
              " [83, 9, 1, 84, 4, 12, 85, 86, 16, 6],\n",
              " [83, 9, 1, 84, 4, 12, 85, 86, 16, 6, 87]]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "8lr5MiQZOQQx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKcYzKINUCqt",
        "outputId": "4747ea4a-7e73-433d-8db5-1144dd9e301d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "vkZZGWTfTeZP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3PvqWMHT7G3",
        "outputId": "2bcbb540-d5c2-4fca-baa2-1bc69802e526"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  1, 21],\n",
              "       [ 0,  0,  0, ...,  1, 21,  2],\n",
              "       [ 0,  0,  0, ..., 21,  2, 22],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 85, 86, 16],\n",
              "       [ 0,  0,  0, ..., 86, 16,  6],\n",
              "       [ 0,  0,  0, ..., 16,  6, 87]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "-mXCxNOqUmoW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7EX8AUIVLpw",
        "outputId": "61c8f598-4320-427d-be92-fffecb17bb7b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  0,  1],\n",
              "       [ 0,  0,  0, ...,  0,  1, 21],\n",
              "       [ 0,  0,  0, ...,  1, 21,  2],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 12, 85, 86],\n",
              "       [ 0,  0,  0, ..., 85, 86, 16],\n",
              "       [ 0,  0,  0, ..., 86, 16,  6]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djQ-7TTSVTkQ",
        "outputId": "617ea34d-793c-44fe-cb7f-3c051c2e84bb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21,  2, 22, 23, 24, 25, 26, 10,  5, 28,  6, 29, 10,  7, 30, 31,  3,\n",
              "       11,  1, 12, 13, 32,  2, 33,  4, 34, 35,  2, 36,  8, 37, 14,  2, 38,\n",
              "       15, 16, 17, 39, 18, 40,  3, 42, 43, 44, 45,  6, 46,  3, 47,  5, 48,\n",
              "       49,  2, 50,  8,  1,  1, 51, 52,  4, 53, 54, 55, 57,  3, 58, 59, 60,\n",
              "       61, 15, 14,  2, 62, 18, 63, 64, 65, 66, 67, 68,  8,  1, 69, 70, 71,\n",
              "       72, 17, 19,  1,  4, 20, 73,  7,  9, 19,  1, 75, 76, 77, 20,  7, 78,\n",
              "       13, 79, 11, 80, 81,  5, 82,  9,  1, 84,  4, 12, 85, 86, 16,  6, 87],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTseIyqMfkYd",
        "outputId": "7dadca10-81f8-4f4e-cdd2-0be6c85f45d5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': 1,\n",
              " 'a': 2,\n",
              " 'on': 3,\n",
              " 'to': 4,\n",
              " 'or': 5,\n",
              " 'for': 6,\n",
              " 'decisions': 7,\n",
              " 'of': 8,\n",
              " 'this': 9,\n",
              " 'life': 10,\n",
              " 'some': 11,\n",
              " 'be': 12,\n",
              " 'it': 13,\n",
              " 'buying': 14,\n",
              " 'after': 15,\n",
              " 'considering': 16,\n",
              " 'the': 17,\n",
              " 'and': 18,\n",
              " 'raw': 19,\n",
              " 'make': 20,\n",
              " 'plays': 21,\n",
              " 'vital': 22,\n",
              " 'role': 23,\n",
              " 'in': 24,\n",
              " 'our': 25,\n",
              " 'everyday': 26,\n",
              " 'directly': 27,\n",
              " 'indirectly': 28,\n",
              " 'daily': 29,\n",
              " 'we': 30,\n",
              " 'depend': 31,\n",
              " 'choosing': 32,\n",
              " 'novel': 33,\n",
              " 'read': 34,\n",
              " 'from': 35,\n",
              " 'list': 36,\n",
              " 'books': 37,\n",
              " 'thing': 38,\n",
              " 'budget': 39,\n",
              " 'so': 40,\n",
              " 'have': 41,\n",
              " 'you': 42,\n",
              " 'ever': 43,\n",
              " 'imagined': 44,\n",
              " 'searching': 45,\n",
              " 'something': 46,\n",
              " 'google': 47,\n",
              " 'yahoo': 48,\n",
              " 'generates': 49,\n",
              " 'lot': 50,\n",
              " 'is': 51,\n",
              " 'essential': 52,\n",
              " 'analyze': 53,\n",
              " 'user': 54,\n",
              " 'experiences': 55,\n",
              " 'getting': 56,\n",
              " 'recommendations': 57,\n",
              " 'various': 58,\n",
              " 'e': 59,\n",
              " 'commerce': 60,\n",
              " 'websites': 61,\n",
              " 'product': 62,\n",
              " 'tracking': 63,\n",
              " 'parcels': 64,\n",
              " 'during': 65,\n",
              " 'delivery': 66,\n",
              " 'are': 67,\n",
              " 'part': 68,\n",
              " 'analytics': 69,\n",
              " 'which': 70,\n",
              " 'involves': 71,\n",
              " 'analyzing': 72,\n",
              " 'informed': 73,\n",
              " 'but': 74,\n",
              " 'does': 75,\n",
              " 'not': 76,\n",
              " 'help': 77,\n",
              " 'if': 78,\n",
              " 'has': 79,\n",
              " 'redundancy': 80,\n",
              " 'inconsistency': 81,\n",
              " 'inaccuracy': 82,\n",
              " 'therefore': 83,\n",
              " 'needs': 84,\n",
              " 'cleaned': 85,\n",
              " 'before': 86,\n",
              " 'analysis': 87}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=88)"
      ],
      "metadata": {
        "id": "BikWaC4fg0y1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62RBz7AwhCe0",
        "outputId": "69bbd09e-143d-4cf9-e788-6a38fb1844ed"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119, 88)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vns3pQFNhSFq",
        "outputId": "46ab1b28-5fad-45ae-a80a-a61f0a5d2b1f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Building**"
      ],
      "metadata": {
        "id": "hyFeZ9R1FPzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense"
      ],
      "metadata": {
        "id": "3JGGM9PlhTqK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(88,100,input_length=33))\n",
        "model.add(GRU(150))\n",
        "model.add(Dense(88,activation='softmax'))"
      ],
      "metadata": {
        "id": "lIbSgCVHFbO_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bDG251CFFgHX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JgOuNxiGPgH",
        "outputId": "0b05c2b9-d430-460f-d56e-82e5bdc111c2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 33, 100)           8800      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 150)               113400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 88)                13288     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 135488 (529.25 KB)\n",
            "Trainable params: 135488 (529.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWHNq7UQIW6T",
        "outputId": "17768221-39f1-430b-95dc-184d46d51d36"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agZhL4gCMRgx",
        "outputId": "f6acaa21-7ff6-4936-d43e-1ced0288bbad"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119, 88)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpx97gBhMS2y",
        "outputId": "01057bcf-f81f-4dd9-9f4b-96a13b02184b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 3s 56ms/step - loss: 4.4774 - accuracy: 0.0336\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 4.4554 - accuracy: 0.1092\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 4.4349 - accuracy: 0.0924\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 4.4113 - accuracy: 0.0756\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 4.3769 - accuracy: 0.0588\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 101ms/step - loss: 4.3157 - accuracy: 0.0588\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 4.2417 - accuracy: 0.0588\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 86ms/step - loss: 4.1831 - accuracy: 0.0588\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 4.1449 - accuracy: 0.0672\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 4.0984 - accuracy: 0.0924\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 4.0478 - accuracy: 0.1092\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 3.9894 - accuracy: 0.1092\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 3.9039 - accuracy: 0.1008\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 3.7971 - accuracy: 0.0924\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 3.6881 - accuracy: 0.1176\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 3.5820 - accuracy: 0.1597\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 3.4601 - accuracy: 0.1849\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 3.3396 - accuracy: 0.1681\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 3.2184 - accuracy: 0.2185\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 3.1013 - accuracy: 0.2521\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 2.9691 - accuracy: 0.2521\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 2.8330 - accuracy: 0.3277\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 2.6954 - accuracy: 0.3445\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 2.5597 - accuracy: 0.3529\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 2.4174 - accuracy: 0.4118\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 2.2869 - accuracy: 0.4622\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 2.1467 - accuracy: 0.4874\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 2.0285 - accuracy: 0.6050\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 1.8838 - accuracy: 0.6387\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 1.7676 - accuracy: 0.6807\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 1.6420 - accuracy: 0.7647\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 1.5203 - accuracy: 0.7983\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 1.4196 - accuracy: 0.8151\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.3057 - accuracy: 0.8571\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 1.2103 - accuracy: 0.8908\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 1.1121 - accuracy: 0.9076\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 1.0322 - accuracy: 0.9328\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.9410 - accuracy: 0.9496\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.8655 - accuracy: 0.9580\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.7989 - accuracy: 0.9664\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.7357 - accuracy: 0.9664\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 0.6738 - accuracy: 0.9748\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.6204 - accuracy: 0.9832\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.5708 - accuracy: 0.9916\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.5281 - accuracy: 0.9916\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.4856 - accuracy: 0.9916\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.4491 - accuracy: 0.9832\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.4157 - accuracy: 0.9916\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.3864 - accuracy: 0.9916\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 0.3605 - accuracy: 0.9832\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.3349 - accuracy: 0.9916\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.3126 - accuracy: 0.9916\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.2927 - accuracy: 0.9832\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.2739 - accuracy: 0.9916\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.2582 - accuracy: 0.9916\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 0.2430 - accuracy: 0.9916\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 0.2283 - accuracy: 0.9916\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 0.2164 - accuracy: 0.9916\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.2042 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.1940 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.1833 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.1735 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.1651 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.1571 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.1499 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.1430 - accuracy: 0.9916\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.1360 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.1299 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.1244 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.1197 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.1148 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 0.1095 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.1053 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.1009 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.0967 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0927 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0895 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0859 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.0837 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0791 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0763 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0743 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0709 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.0687 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0661 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0636 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.0617 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 0.0595 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0576 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0558 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0542 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 0.0521 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0506 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0490 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0461 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0449 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0435 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 65ms/step - loss: 0.0422 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0410 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7941f7d01c90>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test the model**"
      ],
      "metadata": {
        "id": "TQPjc7WqMqxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Data\"\n",
        "\n",
        "# tokenization\n",
        "token_text = tokenizer.texts_to_sequences([text2])[0]\n",
        "# padding\n",
        "padded_text = pad_sequences([token_text], maxlen=33, padding='pre')\n",
        "# model prediction\n",
        "model.predict(padded_text)"
      ],
      "metadata": {
        "id": "5MUI7BxvMW-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8877754-fbd8-4400-9a64-e16a66cba4ed"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 410ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.0677977e-06, 6.1152481e-05, 1.0448781e-04, 2.3667780e-03,\n",
              "        5.8708010e-05, 1.2628391e-03, 1.3012551e-04, 1.7305845e-05,\n",
              "        1.1776635e-05, 1.7395497e-03, 5.8327187e-05, 8.2559840e-05,\n",
              "        5.6825747e-04, 2.4589870e-04, 1.1469553e-05, 3.7034465e-06,\n",
              "        7.6273232e-06, 1.0099918e-05, 6.3487994e-07, 6.1588573e-05,\n",
              "        2.3713653e-06, 9.0414077e-01, 7.7031102e-05, 2.9475777e-06,\n",
              "        2.2280921e-05, 1.9273153e-05, 1.4253704e-06, 8.1640983e-06,\n",
              "        3.7890140e-04, 5.5823443e-06, 1.2398397e-05, 1.7130280e-05,\n",
              "        8.8532433e-06, 4.9965806e-06, 2.4130117e-07, 2.8597823e-08,\n",
              "        8.0519257e-08, 7.7693551e-07, 3.6677756e-07, 5.7656007e-07,\n",
              "        6.9117959e-07, 3.9688516e-06, 2.1739674e-03, 6.1621855e-04,\n",
              "        1.8832502e-04, 9.8869668e-06, 5.6816643e-06, 1.7634580e-04,\n",
              "        3.2437088e-06, 1.4987162e-05, 5.1158954e-06, 7.3678248e-02,\n",
              "        2.5175570e-04, 4.9333557e-06, 3.7150562e-06, 2.0534007e-06,\n",
              "        5.3603694e-06, 3.1835118e-03, 4.6246353e-04, 2.8802928e-05,\n",
              "        1.7482726e-05, 1.2884998e-06, 3.6041791e-07, 8.6472085e-08,\n",
              "        1.0572068e-06, 4.9467559e-07, 1.7871791e-06, 4.3359012e-07,\n",
              "        2.4089961e-06, 5.7628501e-04, 1.9531572e-05, 5.1166471e-06,\n",
              "        1.0296626e-05, 7.3795070e-07, 4.0661639e-06, 2.6531795e-03,\n",
              "        3.3299624e-05, 1.6899765e-05, 5.1512920e-06, 5.8660685e-06,\n",
              "        1.0658985e-05, 2.1794776e-05, 6.8066700e-05, 5.6994541e-06,\n",
              "        4.1749231e-03, 2.8704048e-05, 2.1409714e-06, 7.4494318e-07]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pos = np.argmax(model.predict(padded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJLGl4HSNjE0",
        "outputId": "a68a5717-27c2-4d2c-9ec0-7ca49217a18c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBUE1sHnNEO6",
        "outputId": "8a06c136-7995-47f5-b73d-2b83f6f703ed"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': 1,\n",
              " 'a': 2,\n",
              " 'on': 3,\n",
              " 'to': 4,\n",
              " 'or': 5,\n",
              " 'for': 6,\n",
              " 'decisions': 7,\n",
              " 'of': 8,\n",
              " 'this': 9,\n",
              " 'life': 10,\n",
              " 'some': 11,\n",
              " 'be': 12,\n",
              " 'it': 13,\n",
              " 'buying': 14,\n",
              " 'after': 15,\n",
              " 'considering': 16,\n",
              " 'the': 17,\n",
              " 'and': 18,\n",
              " 'raw': 19,\n",
              " 'make': 20,\n",
              " 'plays': 21,\n",
              " 'vital': 22,\n",
              " 'role': 23,\n",
              " 'in': 24,\n",
              " 'our': 25,\n",
              " 'everyday': 26,\n",
              " 'directly': 27,\n",
              " 'indirectly': 28,\n",
              " 'daily': 29,\n",
              " 'we': 30,\n",
              " 'depend': 31,\n",
              " 'choosing': 32,\n",
              " 'novel': 33,\n",
              " 'read': 34,\n",
              " 'from': 35,\n",
              " 'list': 36,\n",
              " 'books': 37,\n",
              " 'thing': 38,\n",
              " 'budget': 39,\n",
              " 'so': 40,\n",
              " 'have': 41,\n",
              " 'you': 42,\n",
              " 'ever': 43,\n",
              " 'imagined': 44,\n",
              " 'searching': 45,\n",
              " 'something': 46,\n",
              " 'google': 47,\n",
              " 'yahoo': 48,\n",
              " 'generates': 49,\n",
              " 'lot': 50,\n",
              " 'is': 51,\n",
              " 'essential': 52,\n",
              " 'analyze': 53,\n",
              " 'user': 54,\n",
              " 'experiences': 55,\n",
              " 'getting': 56,\n",
              " 'recommendations': 57,\n",
              " 'various': 58,\n",
              " 'e': 59,\n",
              " 'commerce': 60,\n",
              " 'websites': 61,\n",
              " 'product': 62,\n",
              " 'tracking': 63,\n",
              " 'parcels': 64,\n",
              " 'during': 65,\n",
              " 'delivery': 66,\n",
              " 'are': 67,\n",
              " 'part': 68,\n",
              " 'analytics': 69,\n",
              " 'which': 70,\n",
              " 'involves': 71,\n",
              " 'analyzing': 72,\n",
              " 'informed': 73,\n",
              " 'but': 74,\n",
              " 'does': 75,\n",
              " 'not': 76,\n",
              " 'help': 77,\n",
              " 'if': 78,\n",
              " 'has': 79,\n",
              " 'redundancy': 80,\n",
              " 'inconsistency': 81,\n",
              " 'inaccuracy': 82,\n",
              " 'therefore': 83,\n",
              " 'needs': 84,\n",
              " 'cleaned': 85,\n",
              " 'before': 86,\n",
              " 'analysis': 87}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "  if index==pos:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emawpNAINzRC",
        "outputId": "2bd27892-b51d-49b9-efcd-820ff58b883e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"Data plays a vital\"\n",
        "\n",
        "# tokenization\n",
        "token_text3 = tokenizer.texts_to_sequences([text3])[0]\n",
        "# padding\n",
        "padded_text3 = pad_sequences([token_text3], maxlen=33, padding='pre')\n",
        "# model prediction\n",
        "model.predict(padded_text3)\n",
        "\n",
        "pos3 = np.argmax(model.predict(padded_text3))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index==pos3:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbbZiNtGOKjZ",
        "outputId": "83b82756-022f-41e0-8391-4cb7f1ca0f93"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "role\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"Data science is essential to analyze\"\n",
        "\n",
        "# tokenization\n",
        "token_text3 = tokenizer.texts_to_sequences([text3])[0]\n",
        "# padding\n",
        "padded_text3 = pad_sequences([token_text3], maxlen=33, padding='pre')\n",
        "# model prediction\n",
        "model.predict(padded_text3)\n",
        "\n",
        "pos3 = np.argmax(model.predict(padded_text3))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index==pos3:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYdZ-2gtOiHp",
        "outputId": "53d25cd9-98bc-4f86-8d5d-029ed7c921da"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "analyze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"exploration of data is\"\n",
        "\n",
        "# tokenization\n",
        "token_text3 = tokenizer.texts_to_sequences([text3])[0]\n",
        "# padding\n",
        "padded_text3 = pad_sequences([token_text3], maxlen=33, padding='pre')\n",
        "# model prediction\n",
        "model.predict(padded_text3)\n",
        "\n",
        "pos3 = np.argmax(model.predict(padded_text3))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index==pos3:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MmQ5TJcPoRz",
        "outputId": "c9b912c1-f4f8-45a5-c244-2fbe862e0ee0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "essential\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"youtube is a vital\"\n",
        "\n",
        "# tokenization\n",
        "token_text3 = tokenizer.texts_to_sequences([text3])[0]\n",
        "# padding\n",
        "padded_text3 = pad_sequences([token_text3], maxlen=33, padding='pre')\n",
        "# model prediction\n",
        "model.predict(padded_text3)\n",
        "\n",
        "pos3 = np.argmax(model.predict(padded_text3))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index==pos3:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr-bG-dRU3jk",
        "outputId": "73615aaf-0fbc-4f9b-de14-f76eda3e3fab"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "role\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Improvements:**\n",
        "\n",
        "1. Increasing the LSTM units\n",
        "2. Increasing the Dense layers\n",
        "3. Increasing the data\n",
        "4. Using a pre-trained model on a larger corpus of data\n",
        "5. Hyper parameter optimization"
      ],
      "metadata": {
        "id": "m9yb7MU_PFyi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Savwt9c9O1Yx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}